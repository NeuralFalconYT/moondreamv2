{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Install and Auto restart\n",
        "!pip install accelerate==0.32.1\n",
        "!pip install huggingface-hub==0.24.6\n",
        "!pip install Pillow==10.4.0\n",
        "# !pip install torch==2.3.1\n",
        "# !pip install torchvision==0.18.1\n",
        "!pip install transformers==4.44.0\n",
        "!pip install einops==0.8.0\n",
        "!pip install gradio==4.36.1\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import time\n",
        "time.sleep(6)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9TkZveLZjkab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- Play the audio { display-mode: \"form\" }\n",
        "\n",
        "# !git clone https://github.com/vikhyat/moondream.git\n",
        "# %cd /content/moondream\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from PIL import Image\n",
        "\n",
        "# model_id = \"vikhyatk/moondream2\"\n",
        "# revision = \"2024-03-13\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id, trust_remote_code=True, revision=revision\n",
        "# )\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
        "# from PIL import Image\n",
        "# image = Image.open('/content/monalisa.jpg')\n",
        "# image = Image.open('/content/monalisa.jpg')\n",
        "# enc_image = model.encode_image(image)\n",
        "# print(model.answer_question(enc_image, \"Describe this image.\", tokenizer))\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then run the cell below</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "TtcASW2W4prb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "48d6c0e5-bdae-4123-be2a-a549dab02fbd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<b>Press play on the music player to keep the tab alive, then run the cell below</b><br/>\n",
              "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downlod and Config MoonDream\n",
        "import torch\n",
        "import re\n",
        "import time\n",
        "# from moondream.moondream import detect_device, LATEST_REVISION\n",
        "from threading import Thread\n",
        "from transformers import TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    use_cpu=False\n",
        "else:\n",
        "    use_cpu=True\n",
        "# Specify whether to use CPU or GPU\n",
        "# use_cpu = True  # Change to True if you want to use CPU\n",
        "\n",
        "if use_cpu:\n",
        "  device = torch.device(\"cpu\")\n",
        "  dtype = torch.float32\n",
        "else:\n",
        "  device=torch.device(\"cuda\")\n",
        "  dtype=torch.float16\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# revision = \"2024-03-13\"\n",
        "revision = \"2024-08-26\"\n",
        "\n",
        "model_id = \"vikhyatk/moondream2\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, revision=LATEST_REVISION)\n",
        "# moondream = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id, trust_remote_code=True, revision=LATEST_REVISION\n",
        "# ).to(device=device, dtype=dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
        "\n",
        "moondream = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, trust_remote_code=True, revision=revision\n",
        ").to(device=device, dtype=dtype)\n",
        "moondream.eval()\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "r37gqp7JcI9n",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title utils for graido\n",
        "def answer_question(image, prompt):\n",
        "    image_embeds = moondream.encode_image(image)\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
        "    thread = Thread(\n",
        "        target=moondream.answer_question,\n",
        "        kwargs={\n",
        "            \"image_embeds\": image_embeds,\n",
        "            \"question\": prompt,\n",
        "            \"tokenizer\": tokenizer,\n",
        "            \"streamer\": streamer,\n",
        "        },\n",
        "    )\n",
        "    thread.start()\n",
        "\n",
        "    buffer = \"\"\n",
        "    for new_text in streamer:\n",
        "        clean_text = re.sub(\"<$|END$\", \"\", new_text)\n",
        "        buffer += clean_text\n",
        "        yield buffer.strip(\"<END\")\n",
        "def get_answer(prompt,image):\n",
        "  answer = []\n",
        "  for text in answer_question(image, prompt):\n",
        "      answer.append(text)\n",
        "\n",
        "  if len(answer) == 0:\n",
        "      answer.append(\"Nothing Found\")\n",
        "\n",
        "  return answer[-1]\n",
        "import uuid\n",
        "def random_image_name():\n",
        "  random_uuid = uuid.uuid4()\n",
        "  image_extension = \".jpg\"\n",
        "  random_image_name = str(random_uuid)[:8] + image_extension\n",
        "  return random_image_name\n",
        "import shutil\n",
        "import os\n",
        "import uuid\n",
        "from PIL import Image\n",
        "\n",
        "if not os.path.exists(\"/content/upload\"):\n",
        "    os.mkdir(\"/content/upload\")\n",
        "\n",
        "def process_upload_image(prompt, gradio_image):\n",
        "    print(gradio_image)\n",
        "    print(type(gradio_image))\n",
        "    try:\n",
        "        # Handle PIL format image\n",
        "        image = Image.open(gradio_image)\n",
        "        # Generate a random image name using UUID\n",
        "        image_name = random_image_name()\n",
        "        # Save the image to the upload directory\n",
        "        copy_image_path = os.path.join(\"/content/upload\", image_name)\n",
        "        print(f\"Upload Image Saved at {copy_image_path}\")\n",
        "        image.save(copy_image_path)\n",
        "        answer=get_answer(prompt,image)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        copy_image_path=f\"/content/upload/{random_image_name()}\"\n",
        "        gradio_image.save(copy_image_path)\n",
        "        print(f\"Upload Image Saved at {copy_image_path}\")\n",
        "        answer=get_answer(prompt,gradio_image)\n",
        "        return answer\n",
        "\n",
        "# # Example usage:\n",
        "# gradio_image_path = \"/content/monalisa.jpg\"  # Replace with the actual path to your image\n",
        "# prompt = \"What's going on? Respond with a single sentence.\"\n",
        "# process_upload_image(prompt, gradio_image_path)\n",
        "# gradio_image_path = Image.open(\"/content/monalisa.jpg\")\n",
        "# process_upload_image(prompt, gradio_image_path)"
      ],
      "metadata": {
        "id": "Ah4V-OqbgDfS",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gradio API Interface\n",
        "username = 'admin'  # @param {type: \"string\"}\n",
        "password = 'admin'  # @param {type: \"string\"}\n",
        "Debug = True  # @param {type: \"boolean\"}\n",
        "\n",
        "import gradio as gr\n",
        "image_inputs=[gr.Textbox(label=\"Write Prompt\",value=\"What's going on? Respond with a single sentence.\"),gr.Image(type='pil',label=\"Upload a Image\")]\n",
        "# image_outputs=[gr.Textbox(label=\"Result\")]\n",
        "image_outputs=[gr.Textbox(label=\"Response\")]\n",
        "\n",
        "app_demo = gr.Interface(fn=process_upload_image, inputs=image_inputs,outputs=image_outputs , title=\"MoonDream\")\n",
        "app_demo.queue().launch(share=True,debug=Debug,auth=(username, password))"
      ],
      "metadata": {
        "id": "3bXnGwosiSAf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}